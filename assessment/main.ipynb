{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "marimo": {
     "name": "setup"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as ptch\n",
    "import scipy.signal as si\n",
    "import scipy.stats as st\n",
    "import scipy.optimize as op\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.preprocessing as pp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, optimizers\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, ReLU, MaxPooling1D, LSTM, Dropout, Dense, Bidirectional, Flatten, GlobalAveragePooling1D, Reshape\n",
    "from functools import partial\n",
    "\n",
    "CSV_COLS = ['timestamp', 'back_x', 'back_y', 'back_z', 'thigh_x', 'thigh_y', 'thigh_z', 'label']\n",
    "ACTIVITIES = ['walking', 'running', 'shuffling', 'standing', 'sitting', 'lying', 'stairs']  \n",
    "UNWANTED_LABELS = [10, 13, 14, 130, 140]\n",
    "BACK_AXES = ['back_x', 'back_y', 'back_z']\n",
    "THIGH_AXES = ['thigh_x', 'thigh_y', 'thigh_z']\n",
    "AXES = np.concatenate((BACK_AXES, THIGH_AXES))\n",
    "BACK_X_FEATURES = ['back_x_10p', 'back_x_median', 'back_x_90p', 'back_x_std']\n",
    "BACK_Y_FEATURES = ['back_y_10p', 'back_y_median', 'back_y_90p', 'back_y_std']\n",
    "BACK_Z_FEATURES = ['back_z_10p', 'back_z_median', 'back_z_90p', 'back_z_std']\n",
    "BACK_ENMO_MS = ['back_enmo_median', 'back_enmo_std']\n",
    "BACK_ENMO_P = ['back_enmo_10p', 'back_enmo_90p']\n",
    "BACK_ENMO_FEATURES = BACK_ENMO_MS + BACK_ENMO_P\n",
    "BACK_FEATURES = BACK_X_FEATURES + BACK_Y_FEATURES + BACK_Z_FEATURES + BACK_ENMO_FEATURES\n",
    "BACK_10_FEATURES = BACK_X_FEATURES + BACK_Z_FEATURES + BACK_ENMO_MS\n",
    "THIGH_X_FEATURES = ['thigh_x_10p', 'thigh_x_median', 'thigh_x_90p', 'thigh_x_std']\n",
    "THIGH_Y_FEATURES = ['thigh_y_10p', 'thigh_y_median', 'thigh_y_90p', 'thigh_y_std']\n",
    "THIGH_Z_FEATURES = ['thigh_z_10p', 'thigh_z_median', 'thigh_z_90p', 'thigh_z_std']\n",
    "THIGH_ENMO_MS = ['thigh_enmo_median', 'thigh_enmo_std']\n",
    "THIGH_ENMO_P = ['thigh_enmo_10p', 'thigh_enmo_90p']\n",
    "THIGH_ENMO_FEATURES = THIGH_ENMO_MS + THIGH_ENMO_P\n",
    "THIGH_FEATURES = THIGH_X_FEATURES + THIGH_X_FEATURES + THIGH_Z_FEATURES + THIGH_ENMO_FEATURES\n",
    "THIGH_10_FEATURES = THIGH_X_FEATURES + THIGH_Z_FEATURES + THIGH_ENMO_MS\n",
    "ALL_FEATURES = BACK_FEATURES + THIGH_FEATURES\n",
    "SENSOR_THRESHOLD = 8\n",
    "SAMPLE_RATE = 50\n",
    "WINDOW_SIZE = 2\n",
    "WINDOW_OVERLAP = 1\n",
    "INPUT_SHAPE = (SAMPLE_RATE * WINDOW_SIZE, len(AXES))\n",
    "EPOCHS = 10\n",
    "pd.set_option('display.max_rows', 100)\n",
    "label_encoder = pp.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Data Pre-processing and exploration\n",
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {
    "marimo": {
     "name": "*load_data"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "        \"\"\"Load data from csv files in a filepath to a dictionary of dataframes.\n",
    "\n",
    "        Args:\n",
    "            filepath: The name of the filepath to load data from.\n",
    "\n",
    "        Returns:\n",
    "            A new dictionary of dataframes, keyed by the filename without the extension.\n",
    "        \"\"\"\n",
    "        _dataframes = []\n",
    "        for filename in os.listdir(filepath):\n",
    "            if filename.endswith('.csv'):\n",
    "                _dataframe = pd.read_csv(filepath+'/'+filename, usecols=CSV_COLS, parse_dates=[0])\n",
    "                _dataframe['sensor'] = os.path.splitext(os.path.basename(filename))[0]\n",
    "                _dataframes.append(_dataframe)\n",
    "        return pd.concat(objs=_dataframes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data('./MLT-CW-Dataset')\n",
    "test_data = load_data('./MLT-CW-Dataset/test-set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Report the class balance of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training dataframe shape: {train_data.shape}')\n",
    "print(f'Testing dataframe shape: {test_data.shape}')\n",
    "print(f'Columns: {train_data.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the class balance of the whole dataset.\n",
    "_value_counts = train_data['label'].value_counts().sort_values(ascending=False)\n",
    "_value_counts.plot(kind='bar', figsize=(8, 5))\n",
    "plt.title('Class Distribution in Training Data')\n",
    "plt.xlabel('Activities')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Write a function to drop data labelled with any type of cycling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "name": "*drop_labelled_rows"
    }
   },
   "outputs": [],
   "source": [
    "def drop_labelled_rows(dataframe, labels):\n",
    "    \"\"\"Drop rows from a dataframe based on row label inplace, i.e. the dataframe is modified.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe.\n",
    "        labels: An array of string label names.\n",
    "\n",
    "    Returns:\n",
    "        A new dictionary of dataframes, keyed by the filename without the extension.\n",
    "    \"\"\"\n",
    "    mask = dataframe['label'].isin(labels)\n",
    "    return dataframe[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Apply it to the dataset and report the new dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_cycling = drop_labelled_rows(train_data, UNWANTED_LABELS)\n",
    "test_unwanted_cycling = drop_labelled_rows(test_data, UNWANTED_LABELS)\n",
    "print(f'Training dataframe shape: {unwanted_cycling.shape}')\n",
    "print(f'Testing dataframe shape: {test_unwanted_cycling.shape}')\n",
    "del train_data\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Write a function to merge stairs in a single class with code 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "name": "*replace_values"
    }
   },
   "outputs": [],
   "source": [
    "def replace_values(dataframe, column, existing_values, replacement_value):\n",
    "    \"\"\"Replace values of a column in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataframe: The dataframe to be modified.\n",
    "        column: A string name of the column to modify values in.\n",
    "        existing_values: An array of strings to be replaced.\n",
    "        replacement_value: A string to substitute.\n",
    "\n",
    "    Returns:\n",
    "        A new dataframe with the column values replaced.\n",
    "    \"\"\"\n",
    "    new_df = dataframe.copy()\n",
    "    # Replace the existing values in the specified column with the replacement_value.\n",
    "    new_df[column] = new_df[column].replace(existing_values, replacement_value)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Apply it to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stairs = replace_values(unwanted_cycling, 'label', [4,5], 9)\n",
    "test_merged_stairs = replace_values(test_unwanted_cycling, 'label', [4,5], 9)\n",
    "print(f'Training dataframe shape after merging stairs: {unwanted_cycling.shape}')\n",
    "print(f'Testing dataframe shape after merging stairs: {test_unwanted_cycling.shape}')\n",
    "del unwanted_cycling\n",
    "del test_unwanted_cycling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "_value_counts = merged_stairs['label'].value_counts().sort_values(ascending=False)\n",
    "_value_counts.plot(kind='bar', figsize=(8, 5))\n",
    "plt.title('Class Distribution after cycling removed and stairs merged')\n",
    "plt.xlabel('Activities')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Use appropriate sampling and visualisations for Walking.\n",
    "Start by selecting a sensor set and a sample of rows, limited to a meaningful number, say 8 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "_walking_viz = merged_stairs[(merged_stairs['label'] == 1) & (merged_stairs['sensor'] == 'S012')].iloc[::1].head(400)\n",
    "_fig, _ax = plt.subplots(6, 1, figsize=(10, 12), sharex=True)\n",
    "for _i, _col in enumerate(AXES):\n",
    "    _ax[_i].plot(_walking_viz['timestamp'], _walking_viz[_col], linewidth=0.5, label=\"Raw Data\")\n",
    "    # Use built-in Savitzky-Golay filter for preserving shape while denoising the data.\n",
    "    _ax[_i].plot(_walking_viz['timestamp'], si.savgol_filter(_walking_viz[_col], window_length=5, polyorder=1), linewidth=0.8, label=\"Savitzky-Golay Filter\")\n",
    "    _ax[_i].set_title(_col)\n",
    "    _ax[_i].set_ylabel('Acceleration')\n",
    "\n",
    "_ax[-1].legend(loc=\"upper right\")\n",
    "_ax[-1].set_xlabel('Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del _walking_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Analyse and report the data quality of the S007 subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### There is an issue with all sensors - some accelerometer data is constant across timestamps\n",
    "This is particularly true for S007, notably because all axes of both sensors show a constant.\n",
    "Identify by looking for discontinuities of values, grouping by sensor and then axis. Once groups of values are identified, aggregate each group to capture the first and last timestamps of the series, and the number of rows in between. There are around 250 instances where a sensor is reading a perfectly constant acceleration of more than 1 second for a sampling frequency of 50Hz. It would be impossible for a human to achieve this, and must be caused by the sensor getting 'stuck' in a particular position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "_results = {}\n",
    "# First group by sensor, then by axis.\n",
    "for sensor_name in merged_stairs['sensor'].unique():\n",
    "    _sensor_df = merged_stairs[merged_stairs['sensor'] == sensor_name].copy().reset_index(drop=True)\n",
    "    if len(_sensor_df) < 101:\n",
    "        continue\n",
    "\n",
    "    _sensor_results = {}\n",
    "    for _col in AXES:\n",
    "        _groups_s = (_sensor_df[_col] != _sensor_df[_col].shift()).cumsum()\n",
    "        _grouped_s = _sensor_df.groupby([_groups_s, _col]).agg(\n",
    "            _start_time=('timestamp', 'first'),\n",
    "            _end_time=('timestamp', 'last'),\n",
    "            _count=('timestamp', 'size')\n",
    "        ).reset_index(drop=True)\n",
    "        _filtered_s = _grouped_s[_grouped_s['_count'] > 100]\n",
    "        _sensor_results[_col] = _filtered_s[['_start_time', '_end_time', '_count']]\n",
    "    _results[sensor_name] = _sensor_results\n",
    "\n",
    "# Flatten to single DataFrame with sensor/axis as first columns.\n",
    "_flattened_rows = []\n",
    "for sensor, axes_data in _results.items():\n",
    "    for axis, df in axes_data.items():\n",
    "        if not df.empty:\n",
    "            # Add sensor and axis columns to each row\n",
    "            df_with_meta = df.copy()\n",
    "            df_with_meta.insert(0, 'axis', axis)\n",
    "            df_with_meta.insert(0, 'sensor', sensor)\n",
    "            _flattened_rows.append(df_with_meta)\n",
    "\n",
    "_flattened_df = pd.concat(_flattened_rows, ignore_index=True).sort_values(ascending=False,by='_count')\n",
    "print (_flattened_df._count.sum())\n",
    "print(_flattened_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Information loss with ENMO\n",
    "There are many data rows where the Euclidean Norm Minus One (ENMO) is less than zero. This could be possible during periods of downward step, where the x-axis (vertical) measured acceleration is less than 1 (i.e. the thigh is being forced down) while y- and z-axis acceleration is close to zero, or could be due to incorrect sensor calibration. Truncating negative values to zero (standard in much research) causes information loss and reduces the ability to identify activities, so for label fitting, plain ENMO is used, not max(ENMO, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "_enmo_mask = (np.linalg.norm(merged_stairs[BACK_AXES], axis=1) - 1) < 0\n",
    "print(f'Percentage of back sensor readings with ENMO < 0 (normally truncated): {np.count_nonzero(_enmo_mask) * 100/len(merged_stairs):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### There are also discontinuities in the data\n",
    "This means that windows can either be a fixed number of rows or length of time, not both. For example, sensor S006 shows a 3-second gap at 00h03m08s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mask = (\n",
    "    (merged_stairs.sensor == 'S006') &\n",
    "    (merged_stairs.timestamp >= pd.Timestamp('2019-01-12 00:03:08.360')) &\n",
    "    (merged_stairs.timestamp <= pd.Timestamp('2019-01-12 00:03:11.610'))\n",
    ")\n",
    "print(merged_stairs.loc[_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Remove the particularly egregious S007 constant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mask = ~(\n",
    "    (merged_stairs.sensor == 'S007') &\n",
    "    (merged_stairs.timestamp >= pd.Timestamp('2019-01-17 00:00:41.840')) &\n",
    "    (merged_stairs.timestamp <= pd.Timestamp('2019-01-17 00:01:25.660'))\n",
    ")\n",
    "merged_without_s007_flat = merged_stairs.loc[_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {},
   "outputs": [],
   "source": [
    "s007_data = merged_without_s007_flat[merged_without_s007_flat.sensor == 'S007']\n",
    "non_s007_data = merged_without_s007_flat[merged_without_s007_flat.sensor != 'S007']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHFh",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Spurious outlier data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NCOB",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "S007 accelerometer data show unreasonable outliers seen in these violin plots; the AX3 sensor is rated to +-8g. However, the means and distribution of the data without outliers is similar, suggesting that only a few timestamps are erroneous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {},
   "outputs": [],
   "source": [
    "_fig, _ax = plt.subplots(ncols=1, nrows=6, figsize=(10, 6), sharex=True)\n",
    "_fig.suptitle('Violin plots of axes for s007 vs non_s007')\n",
    "for _i, _col in enumerate(AXES):\n",
    "    _ax[_i].violinplot(non_s007_data[_col].values.T, vert=False)\n",
    "    _ax[_i].violinplot(s007_data[_col].values.T, vert=False)\n",
    "    _ax[_i].set_xlabel(_col)\n",
    "non_patch = ptch.Patch(color='tab:blue',  alpha=0.6, label='non-S007')\n",
    "s007_patch = ptch.Patch(color='tab:orange', alpha=0.6, label='S007')\n",
    "_fig.suptitle('Violin plots of axes for s007 vs non_s007')\n",
    "_ax[0].legend(handles=[non_patch, s007_patch], loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded dataframes to free memory.\n",
    "del s007_data\n",
    "del non_s007_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TXez",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Remove spurious outliers\n",
    "Outliers are removed by setting any sensor acceleration values where the value is above the rated maximum (8g) for the AX3 accelerometer, in any axis, to null, and then interpolating any nulls linearly and with forward- and backward-fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {
    "marimo": {
     "name": "*interpolate_outliers"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate_outliers(df, columns, threshold, method):\n",
    "    outliers_removed = df.copy()\n",
    "    outliers_removed[columns] = np.where(\n",
    "        outliers_removed[columns].abs() <= threshold,\n",
    "        outliers_removed[columns], np.nan\n",
    "    )\n",
    "    outliers_removed[columns] = outliers_removed[columns].interpolate(method=method).ffill().bfill()\n",
    "    return outliers_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = interpolate_outliers(merged_without_s007_flat, AXES, SENSOR_THRESHOLD, 'linear')\n",
    "test_cleaned = interpolate_outliers(test_merged_stairs, AXES, SENSOR_THRESHOLD, 'linear')\n",
    "cleaned_unique_labels = sorted(cleaned.label.unique())\n",
    "label_encoder.fit(cleaned_unique_labels)\n",
    "del merged_without_s007_flat\n",
    "del merged_stairs\n",
    "del test_merged_stairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlCL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Show distribution of acceleration with outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": [
    "_fig, _ax = plt.subplots(ncols=1, nrows=6, figsize=(10, 6), sharex=True)\n",
    "_fig.suptitle('Violin plots of axes for outliers removed')\n",
    "for _i, _col in enumerate(AXES):\n",
    "    _ax[_i].violinplot(cleaned[_col].values.T, vert=False)\n",
    "    _ax[_i].set_xlabel(_col)\n",
    "_no_outliers = ptch.Patch(color='tab:blue',  alpha=0.6, label='outliers-removed')\n",
    "_ax[0].legend(handles=[_no_outliers], loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wAgl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots of sensor data by X,Y,Z for back, thigh locations\n",
    "_fig, _axes = plt.subplots(2, 3, figsize=(10, 8), sharey=True, sharex=True)\n",
    "_fig.suptitle('Univariate analysis of acceleration by X,Y,Z for activity and back, thigh locations with outliers removed')\n",
    "_labels = sorted(cleaned['label'].unique())\n",
    "for _i, _ax in enumerate(_axes.flatten()):\n",
    "    data = [cleaned[cleaned['label'] == _lbl][AXES[_i]].values for _lbl in _labels]\n",
    "    _bp =_ax.boxplot(data, tick_labels=_labels, showfliers=False, patch_artist=True)\n",
    "    _ax.set_title(AXES[_i])\n",
    "    [_median.set_color('black') for _median in _bp['medians']]\n",
    "    for _patch, _color in zip(_bp['boxes'], plt.cm.tab10.colors):\n",
    "        _patch.set_facecolor(_color)\n",
    "        _patch.set_edgecolor('black')\n",
    "_fig.supxlabel('Activity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rEll",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "It's clear from the distribution in raw accelerometer readings from sensors that some of the activities lead to sensor readings with very distinctive characteristics. For example, class 2 (running) has much greater range in minimum and maximum values compared with class 1 (walking). On the other hand, the values for walking and class 9 (stairs) are very similar, suggesting that simple separation by min, max, mean and standard deviation between these two classes is going to be very difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dGlV",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Report on dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SdmI",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgWD",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned.groupby('label').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yOPj",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "##Write a function that receives the dataset and outputs sliding windows of 2 seconds size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwwy",
   "metadata": {
    "marimo": {
     "name": "*generate_time_windows"
    }
   },
   "outputs": [],
   "source": [
    "def generate_time_windows(df, window_length, overlap):\n",
    "    \"\"\"Generate windows (slices of a dictionary of dataframes). By iterating over a sensor grouping,\n",
    "    no window can span two sensors.\n",
    "\n",
    "    Args: \n",
    "        df: A complete dataframe containing multiple sensor data.\n",
    "        window_length: Length of each window to be created, in seconds.\n",
    "        overlap: Overlap of the next window over the previous, in seconds.\n",
    "\n",
    "    Returns:\n",
    "        An array of windows (dataframes), aggregated across the dictionary of dataframes passed in.\n",
    "    \"\"\"    \n",
    "    window_size = pd.Timedelta(seconds=window_length)\n",
    "    step_size = pd.Timedelta(seconds=(window_length - overlap))\n",
    "    # windows is an array of dataframes.\n",
    "    windows = []\n",
    "    # Iterate by sensor, so that windows can't span sensors (wouldn't make sense).\n",
    "    for sensor_name in df['sensor'].unique():\n",
    "        sensor_df = df[df['sensor'] == sensor_name].copy().reset_index(drop=True)\n",
    "        start_time = sensor_df.timestamp.min()\n",
    "        end_time = sensor_df.timestamp.max()\n",
    "        current_start = start_time\n",
    "        while current_start + window_size <= end_time:\n",
    "            current_end = current_start + window_size\n",
    "            window_slice = sensor_df[(sensor_df.timestamp >= current_start) & (sensor_df.timestamp < current_end)]\n",
    "            # Unfortunately the discontinuities / gaps in sensor data timestamps mean that some time-based slices are empty.\n",
    "            if not window_slice.empty:\n",
    "                windows.append(window_slice)\n",
    "            current_start += step_size\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LJZf",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Report the number of data points with this window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urSm",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_array = generate_time_windows(df=cleaned, window_length=WINDOW_SIZE, overlap=WINDOW_OVERLAP)\n",
    "print(len(windows_array))\n",
    "test_windows_array = generate_time_windows(df=test_cleaned, window_length=WINDOW_SIZE, overlap=WINDOW_OVERLAP)\n",
    "print(len(test_windows_array))\n",
    "print(f'Train data window count: {len(windows_array)}')\n",
    "print(f'Test data window count: {len(test_windows_array)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jxvo",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mWxS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Pipelines\n",
    "## Establish a baseline with an initial set of 10 features\n",
    "A solid superset of features is built in one go. 10 of these are to be selected for baseline evaluation. Looking at boxplot univariate analysis, there are clear differences in max and min values by activity, but some of the data are spurious and truncated at +-8g, so max and min is less effective (and also susceptible to participant body dimensions). Instead, the 10-percentile and 90-percentile is taken, as well as the median and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CcZR",
   "metadata": {
    "marimo": {
     "name": "*stats_from_frame"
    }
   },
   "outputs": [],
   "source": [
    "def stats_from_frame(df, norm_name, column_names):\n",
    "    \"\"\"Calculate stats from a np.array.\n",
    "\n",
    "    Args: \n",
    "        data_frame: The dataframe containing the columns to be aggregated.\n",
    "        norm_name: A string to be used for the norm column names.\n",
    "        column_names: An array of strings containing column names to be used.\n",
    "\n",
    "    Returns:\n",
    "        An array of scalars for 10%, 50% (median), 90%tile, standard deviation for each column, \n",
    "        and the same for the Euclidian norm.\n",
    "        An array of column names for the array of scalars.\n",
    "    \"\"\"\n",
    "    norm_array = np.linalg.norm(df[column_names], axis=1)\n",
    "    stats = []\n",
    "    titles = []\n",
    "    for column in column_names:\n",
    "        # stats.append(df[column_names].quantile([0.1, 0.5, 0.9]))\n",
    "        stats.extend(np.percentile(a=df[column], q=[10, 50, 90]))\n",
    "        # Because the deviation is calculated from the entire data set, not a sample, ddof (degrees of freedom) is 0.\n",
    "        # If ddof=1, then std() returns Nan if only one row processed. Nan value is not allowed in kmeans analysis.\n",
    "        stats.append(df[column].std(ddof=0))\n",
    "        titles.extend([f'{column}_10p', f'{column}_median', f'{column}_90p', f'{column}_std'])\n",
    "    stats.extend(np.percentile(a=norm_array, q=[10, 50, 90], axis=0))\n",
    "    stats.append(norm_array.std(ddof=0))\n",
    "    titles.extend([f'{norm_name}_10p', f'{norm_name}_median', f'{norm_name}_90p', f'{norm_name}_std'])\n",
    "    return stats, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YWSi",
   "metadata": {
    "marimo": {
     "name": "*generate_window_summaries"
    }
   },
   "outputs": [],
   "source": [
    "def generate_window_summaries(windows, norm_name_1, axes_1, norm_name_2, axes_2):\n",
    "    \"\"\"Generate a dataframe containing a summary and statistics for each window.\n",
    "\n",
    "    Args: \n",
    "        windows: An array of windows, where each window is a dataframe.\n",
    "        back_features: An array of arrays containing column names to be used as features, normally data from a set of 3 axes.\n",
    "        back_enmo: An array of arrays containing column names to be used as features.\n",
    "        thigh_features: An array of arrays containing column names to be used as features, normally data from a set of 3 axes.\n",
    "        thigh_enmo: An array of arrays containing column names to be used as features.\n",
    "\n",
    "    Returns:\n",
    "        An array of scalars for mean, standard deviation, min, max for each column, \n",
    "        and the same for the Euclidian norm of the columns combined.\n",
    "    \"\"\"\n",
    "    window_summaries = []\n",
    "    for _window in windows:\n",
    "        stats_1, titles_1 = stats_from_frame(_window, norm_name_1, axes_1)\n",
    "        stats_2, titles_2 = stats_from_frame(_window, norm_name_2, axes_2)\n",
    "        window_summaries.append(np.concatenate((\n",
    "            # First timestamp of window.\n",
    "            [_window.timestamp.min()], \n",
    "            # Separate back and thigh so that the function can generate a single set of norms generically. \n",
    "            stats_1,\n",
    "            stats_2,\n",
    "            # Choose the most popular label for the window.\n",
    "            [_window.label.mode()[0]] \n",
    "        )))\n",
    "    return pd.DataFrame(window_summaries, columns=['timestamp'] + titles_1 + titles_2 + ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zlud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_from_frame(windows_array[0], 'back_enmo', BACK_AXES)\n",
    "window_summaries = generate_window_summaries(windows_array, 'back_enmo', BACK_AXES, 'thigh_enmo', THIGH_AXES)\n",
    "test_window_summaries = generate_window_summaries(test_windows_array, 'back_enmo', BACK_AXES, 'thigh_enmo', THIGH_AXES)\n",
    "print(f'Training data window summaries: {window_summaries.shape}')\n",
    "print(f'Test data window summaries: {test_window_summaries.shape}')\n",
    "del windows_array\n",
    "del test_windows_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tZnO",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_summaries.iloc[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xvXZ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CLip",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Baseline\n",
    "Begin by plotting the inertia (sum of squares of distance from points to corresponding cluster centres) against number of clusters to identify the right number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YECM",
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = window_summaries[THIGH_10_FEATURES]\n",
    "K = range(1, 11)\n",
    "inertias = []\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        init='k-means++',\n",
    "        n_init=100,\n",
    "        random_state=42,\n",
    "    )\n",
    "    kmeans.fit(_data)\n",
    "    inertias.append(kmeans.inertia_)   \n",
    "plt.figure()\n",
    "plt.plot(K, inertias, 'o-')\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow method for KMeans\")\n",
    "plt.xticks(K)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cEAS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "The elbow method clearly shows an inflection point at 3 clusters. In an unsupervised setting, this would be the optimum cluster level, but in this case, an unsupervised method is being applied to labelled data with 7 known labels and so the K-means model is configured for 7 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iXej",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-means model with 7 clusters (since we are targetting assignment to 7 labels).\n",
    "kmeans_model = KMeans(n_clusters=7, init='k-means++', n_init='auto', random_state=42) #, algorithm='elkan'\n",
    "kmeans_model.fit(window_summaries[THIGH_10_FEATURES])\n",
    "# Create a copy of the windows data for k-means processing.\n",
    "_kmeans_data = window_summaries.copy()\n",
    "# Assign a cluster index to each window. Note that the kmeans labels are not the same as the groundtruth labels - \n",
    "# they are an arbritary tag value.\n",
    "_kmeans_data['cluster'] = kmeans_model.labels_\n",
    "_unique_labels = sorted(_kmeans_data.label.unique())\n",
    "# The original confusion matrix.\n",
    "_confusion_raw = sm.confusion_matrix(label_encoder.transform(_kmeans_data.label), _kmeans_data.cluster)\n",
    "\n",
    "# Maximize diagonal sum, so that we can map from cluster tags to groundtruth labels. \n",
    "# Ideally the diagonals (True Positives) have high values, with all of the other cells being low or zero.\n",
    "_row_indices, _col_indices = op.linear_sum_assignment(-1 * _confusion_raw)  \n",
    "# Map original cluster labels to new labels for best alignment.\n",
    "_mapping = {old: new for old, new in zip(_col_indices, _row_indices)}\n",
    "\n",
    "# Apply the remapping to cluster labels.\n",
    "_kmeans_data['cluster_mapped'] = label_encoder.inverse_transform(_kmeans_data['cluster'].map(_mapping))\n",
    "\n",
    "_precision_score = sm.precision_score(_kmeans_data['label'], _kmeans_data['cluster_mapped'], average=None)\n",
    "_recall_score = sm.recall_score(_kmeans_data['label'], _kmeans_data['cluster_mapped'], average=None)\n",
    "for _i, _label in enumerate(_unique_labels):\n",
    "    print(f'For label {_label}: Precision {_precision_score[_i]:.2f}, Recall {_recall_score[_i]:.2f}')\n",
    "\n",
    "# Compute the confusion matrix with remapped clusters.\n",
    "_confusion_mapped = sm.confusion_matrix(_kmeans_data['label'], _kmeans_data['cluster_mapped'])\n",
    "_disp = sm.ConfusionMatrixDisplay(_confusion_mapped, display_labels=_unique_labels)\n",
    "_disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EJmg",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "K-Means 7 clusters, BACK_ENMO_FEATURES, n_clusters=7, init='random', n_init=100, random_state=42.\n",
    "For label 1: Precision 0.82, Recall 0.34\n",
    "For label 2: Precision 1.00, Recall 0.53\n",
    "For label 3: Precision 0.09, Recall 0.15\n",
    "For label 6: Precision 0.00, Recall 0.00\n",
    "For label 7: Precision 0.69, Recall 0.93\n",
    "For label 8: Precision 0.27, Recall 0.22\n",
    "For label 9: Precision 0.11, Recall 0.19\n",
    "\n",
    "K-Means 7 clusters, BACK_ENMO_FEATURES, n_clusters=7, init='k-means++', n_init=100, random_state=42.\n",
    "For label 1: Precision 0.83, Recall 0.34\n",
    "For label 2: Precision 1.00, Recall 0.53\n",
    "For label 3: Precision 0.09, Recall 0.14\n",
    "For label 6: Precision 0.00, Recall 0.00\n",
    "For label 7: Precision 0.68, Recall 0.93\n",
    "For label 8: Precision 0.27, Recall 0.22\n",
    "For label 9: Precision 0.11, Recall 0.18\n",
    "\n",
    "K-Means 7 clusters, BACK_10_FEATURES, n_clusters=7, init='k-means++', n_init=100, random_state=42.\n",
    "For label 1: Precision 0.82, Recall 0.62\n",
    "For label 2: Precision 0.98, Recall 0.94\n",
    "For label 3: Precision 0.00, Recall 0.01\n",
    "For label 6: Precision 0.18, Recall 0.40\n",
    "For label 7: Precision 0.62, Recall 0.42\n",
    "For label 8: Precision 0.99, Recall 0.73\n",
    "For label 9: Precision 0.00, Recall 0.00\n",
    "\n",
    "K-Means 7 clusters, ALL_FEATURES, n_clusters=7, init='k-means++', n_init=100, random_state=42.\n",
    "For label 1: Precision 0.80, Recall 0.69\n",
    "For label 2: Precision 0.98, Recall 0.86\n",
    "For label 3: Precision 0.00, Recall 0.00\n",
    "For label 6: Precision 0.56, Recall 0.98\n",
    "For label 7: Precision 0.98, Recall 0.73\n",
    "For label 8: Precision 0.76, Recall 0.35\n",
    "For label 9: Precision 0.00, Recall 0.00\n",
    "\n",
    "K-Means 7 clusters, THIGH_10_FEATURES, n_clusters=7, init='k-means++', n_init=100, random_state=42.\n",
    "For label 1: Precision 0.75, Recall 0.45\n",
    "For label 2: Precision 0.99, Recall 0.75\n",
    "For label 3: Precision 0.00, Recall 0.00\n",
    "For label 6: Precision 0.62, Recall 0.96\n",
    "For label 7: Precision 0.95, Recall 0.98\n",
    "For label 8: Precision 1.00, Recall 0.55\n",
    "For label 9: Precision 0.06, Recall 0.17\n",
    "\n",
    "K-Means 7 clusters, BACK_10_FEATURES + THIGH_10_FEATURES, n_clusters=7, init='k-means++', n_init=100, random_state=42.\n",
    "For label 1: Precision 0.80, Recall 0.69\n",
    "For label 2: Precision 0.98, Recall 0.87\n",
    "For label 3: Precision 0.00, Recall 0.00\n",
    "For label 6: Precision 0.55, Recall 0.97\n",
    "For label 7: Precision 0.97, Recall 0.75\n",
    "For label 8: Precision 0.99, Recall 0.54\n",
    "For label 9: Precision 0.00, Recall 0.00\n",
    "\n",
    "K-Means 7 clusters, BACK_10_FEATURES + THIGH_10_FEATURES, n_clusters=7, init='k-means++', n_init=1, random_state=42.\n",
    "For label 1: Precision 0.80, Recall 0.38\n",
    "For label 2: Precision 0.99, Recall 0.81\n",
    "For label 3: Precision 0.00, Recall 0.00\n",
    "For label 6: Precision 0.62, Recall 0.96\n",
    "For label 7: Precision 0.97, Recall 0.75\n",
    "For label 8: Precision 0.84, Recall 0.60\n",
    "For label 9: Precision 0.20, Recall 0.74\n",
    "\n",
    "K-Means 7 clusters, BACK_10_FEATURES + THIGH_10_FEATURES, n_clusters=7, init='k-means++', n_init=1, random_state=42, algorithm='elkan'.\n",
    "For label 1: Precision 0.80, Recall 0.38\n",
    "For label 2: Precision 0.99, Recall 0.81\n",
    "For label 3: Precision 0.00, Recall 0.00\n",
    "For label 6: Precision 0.62, Recall 0.96\n",
    "For label 7: Precision 0.97, Recall 0.75\n",
    "For label 8: Precision 0.84, Recall 0.60\n",
    "For label 9: Precision 0.20, Recall 0.74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UmEG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vEBW",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kLmu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IpqN",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dxZZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove inappropriate columns from classification dataframe.\n",
    "_rf_x_train = window_summaries.drop(['timestamp', 'label'], axis=1, inplace=False)\n",
    "# Labels.\n",
    "_rf_y_train = window_summaries['label']  \n",
    "\n",
    "# Instantiate and train Random Forest classifier\n",
    "_rf = RandomForestClassifier(n_estimators=100, random_state=42,min_samples_leaf=1000)\n",
    "_rf.fit(_rf_x_train[ALL_FEATURES], _rf_y_train)\n",
    "\n",
    "# Predict labels for test set.\n",
    "_rf_pred_labels = _rf.predict(test_window_summaries[ALL_FEATURES])\n",
    "# Generate confusion matrix. \n",
    "_confusion_rf = sm.confusion_matrix(test_window_summaries['label'], _rf_pred_labels)\n",
    "# Create a list of display labels.\n",
    "_display_labels = sorted(test_window_summaries['label'].unique())\n",
    "\n",
    "_precision_score = sm.precision_score(test_window_summaries['label'], _rf_pred_labels, average=None)\n",
    "_recall_score = sm.recall_score(test_window_summaries['label'], _rf_pred_labels, average=None)\n",
    "for _i, _label in enumerate(_display_labels):\n",
    "    print(f'For label {_label}: Precision {_precision_score[_i]:.2f}, Recall {_recall_score[_i]:.2f}')\n",
    "\n",
    "# By default, the confusion matrix rows and columns will be sorted by label value, so we must provide the same sorting for display.\n",
    "_disp = sm.ConfusionMatrixDisplay(_confusion_rf, display_labels=_display_labels)\n",
    "_disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "del _rf_x_train\n",
    "del _rf_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dlnW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Refinement\n",
    "Combinations tried to improve classes 3 and 9:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TTti",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "n_estimators=100, ALL_FEATURES\n",
    "For label 1: Precision 0.93, Recall 0.84\n",
    "For label 2: Precision 0.99, Recall 0.96\n",
    "For label 3: Precision 0.33, Recall 0.56\n",
    "For label 6: Precision 0.89, Recall 0.86\n",
    "For label 7: Precision 1.00, Recall 1.00\n",
    "For label 8: Precision 1.00, Recall 1.00\n",
    "For label 9: Precision 0.44, Recall 0.58\n",
    "\n",
    "n_estimators=10, ALL_FEATURES\n",
    "For label 1: Precision 0.92, Recall 0.83\n",
    "For label 2: Precision 0.99, Recall 0.96\n",
    "For label 3: Precision 0.30, Recall 0.53\n",
    "For label 6: Precision 0.87, Recall 0.83\n",
    "For label 7: Precision 1.00, Recall 1.00\n",
    "For label 8: Precision 1.00, Recall 1.00\n",
    "For label 9: Precision 0.42, Recall 0.52\n",
    "\n",
    "n_estimators=100, THIGH_10_FEATURES\n",
    "For label 1: Precision 0.92, Recall 0.91\n",
    "For label 2: Precision 0.96, Recall 0.96\n",
    "For label 3: Precision 0.37, Recall 0.45\n",
    "For label 6: Precision 0.89, Recall 0.87\n",
    "For label 7: Precision 0.96, Recall 0.99\n",
    "For label 8: Precision 0.96, Recall 0.78\n",
    "For label 9: Precision 0.59, Recall 0.52\n",
    "\n",
    "n_estimators=100, THIGH_X_FEATURES\n",
    "For label 1: Precision 0.87, Recall 0.89\n",
    "For label 2: Precision 0.93, Recall 0.95\n",
    "For label 3: Precision 0.31, Recall 0.38\n",
    "For label 6: Precision 0.86, Recall 0.82\n",
    "For label 7: Precision 0.89, Recall 0.93\n",
    "For label 8: Precision 0.57, Recall 0.44\n",
    "For label 9: Precision 0.32, Recall 0.23\n",
    "\n",
    "n_estimators=100, criterion='log_loss', THIGH_X_FEATURES\n",
    "For label 1: Precision 0.87, Recall 0.89\n",
    "For label 2: Precision 0.93, Recall 0.95\n",
    "For label 3: Precision 0.32, Recall 0.38\n",
    "For label 6: Precision 0.86, Recall 0.82\n",
    "For label 7: Precision 0.89, Recall 0.94\n",
    "For label 8: Precision 0.57, Recall 0.41\n",
    "For label 9: Precision 0.32, Recall 0.23\n",
    "\n",
    "n_estimators=100, criterion='gini', max_depth=5, THIGH_X_FEATURES\n",
    "For label 1: Precision 0.86, Recall 0.95\n",
    "For label 2: Precision 0.94, Recall 0.96\n",
    "For label 3: Precision 0.37, Recall 0.43\n",
    "For label 6: Precision 0.89, Recall 0.82\n",
    "For label 7: Precision 0.86, Recall 0.99\n",
    "For label 8: Precision 0.72, Recall 0.15\n",
    "For label 9: Precision 0.00, Recall 0.00\n",
    "\n",
    "n_estimators=100, THIGH_X_FEATURES, min_samples_leaf=1000\n",
    "For label 1: Precision 0.86, Recall 0.94\n",
    "For label 2: Precision 0.90, Recall 0.96\n",
    "For label 3: Precision 0.35, Recall 0.37\n",
    "For label 6: Precision 0.87, Recall 0.84\n",
    "For label 7: Precision 0.85, Recall 0.99\n",
    "For label 8: Precision 0.87, Recall 0.08\n",
    "For label 9: Precision 0.00, Recall 0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RKFZ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IaQp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Neural networks need windows of constant size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IWgg",
   "metadata": {
    "marimo": {
     "name": "*generate_fixed_windows"
    }
   },
   "outputs": [],
   "source": [
    "def generate_fixed_windows(df, sample_rate, window_size, overlap):\n",
    "    \"\"\"Generate fixed size windows (slices of a dataframes).\n",
    "\n",
    "    Args: \n",
    "        dataframes: A dictionary of dataframes, keyed by string (e.g. sensor name).\n",
    "        window_length: Length of each window to be created, in seconds.\n",
    "        overlap: Overlap of the next window over the previous, in seconds.\n",
    "\n",
    "    Returns:\n",
    "        An array of windows (dataframes), aggregated across the dictionary of dataframes passed in.\n",
    "    \"\"\"    \n",
    "    window_rows = int(window_size * sample_rate)\n",
    "    window_advance = int((window_size - overlap) * sample_rate)\n",
    "    windows = []\n",
    "    # Group by sensor, so that no window can span sensors.\n",
    "    for sensor_name in df['sensor'].unique():\n",
    "        sensor_df = df[df['sensor'] == sensor_name].copy().reset_index(drop=True)\n",
    "        start_index = 0\n",
    "        while start_index < len(sensor_df):\n",
    "            window_df = sensor_df[start_index:start_index+window_rows]\n",
    "            # Ensure that all windows are of the same length\n",
    "            if len(window_df) == window_rows:\n",
    "                # extra_timestamps = pd.date_range(\n",
    "                #     start=window_df.index.max()+pd.Timedelta(milliseconds=1), \n",
    "                #     periods=window_rows-len(window_df), \n",
    "                #     freq='ms'\n",
    "                # )\n",
    "                # pd.concat([window_df, pd.DataFrame(np.nan, index=extra_timestamps, columns=window_df.columns)])\n",
    "                windows.append(window_df)\n",
    "            start_index = start_index + window_advance\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fCoF",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_windows_array = generate_fixed_windows(df=cleaned, sample_rate=SAMPLE_RATE, window_size=WINDOW_SIZE, overlap=WINDOW_OVERLAP)\n",
    "test_fixed_windows_array = generate_fixed_windows(df=test_cleaned, sample_rate=SAMPLE_RATE, window_size=WINDOW_SIZE, overlap=WINDOW_OVERLAP)\n",
    "print(len(fixed_windows_array))\n",
    "print(len(test_fixed_windows_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LkGn",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Define neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zVRe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Prepare tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "woaO",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = np.stack([df[AXES].values for df in fixed_windows_array], axis=0)\n",
    "y_train_tensor = np.array([df['label'].mode().iloc[0] for df in fixed_windows_array])\n",
    "x_test_tensor = np.stack([df[AXES].values for df in test_fixed_windows_array], axis=0)\n",
    "y_test_tensor = np.array([df['label'].mode().iloc[0] for df in test_fixed_windows_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HnMC",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplest_CNN_model = models.Sequential([\n",
    "    Input(INPUT_SHAPE),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=len(cleaned_unique_labels), activation='softmax')\n",
    "])\n",
    "\n",
    "simplest_CNN_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "simplest_CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wadT",
   "metadata": {},
   "outputs": [],
   "source": [
    "_history = simplest_CNN_model.fit(x_train_tensor, label_encoder.transform(y_train_tensor), epochs=EPOCHS)\n",
    "_y_pred_probability = simplest_CNN_model.predict(x_test_tensor)\n",
    "_y_pred = label_encoder.inverse_transform(np.argmax(_y_pred_probability, axis=1))\n",
    "\n",
    "_precision_score = sm.precision_score(y_test_tensor, _y_pred, average=None)\n",
    "_recall_score = sm.recall_score(y_test_tensor, _y_pred, average=None)\n",
    "for _i, _label in enumerate(cleaned_unique_labels):\n",
    "    print(f'For label {_label}: Precision {_precision_score[_i]:.2f}, Recall {_recall_score[_i]:.2f}')\n",
    "\n",
    "_cm = sm.confusion_matrix(y_test_tensor, _y_pred)\n",
    "_disp = sm.ConfusionMatrixDisplay(confusion_matrix=_cm, display_labels=label_encoder.classes_)\n",
    "_disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Simplest CNN Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VCRE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hgqU",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_improved = models.Sequential([\n",
    "    Input(INPUT_SHAPE),\n",
    "    Conv1D(filters=32, kernel_size=8, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    GlobalAveragePooling1D(), \n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=len(cleaned_unique_labels), activation='softmax')\n",
    "])\n",
    "cnn_model_improved.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.AdamW(learning_rate=0.001), metrics=['accuracy'])\n",
    "cnn_model_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PSUk",
   "metadata": {},
   "outputs": [],
   "source": [
    "_history = cnn_model_improved.fit(x_train_tensor, label_encoder.transform(y_train_tensor), epochs=EPOCHS)\n",
    "_y_pred_probability = cnn_model_improved.predict(x_test_tensor)\n",
    "_y_pred = label_encoder.inverse_transform(np.argmax(_y_pred_probability, axis=1))\n",
    "\n",
    "_precision_score = sm.precision_score(y_test_tensor, _y_pred, average=None)\n",
    "_recall_score = sm.recall_score(y_test_tensor, _y_pred, average=None)\n",
    "for _i, _label in enumerate(cleaned_unique_labels):\n",
    "    print(f'For label {_label}: Precision {_precision_score[_i]:.2f}, Recall {_recall_score[_i]:.2f}')\n",
    "\n",
    "_cm = sm.confusion_matrix(y_test_tensor, _y_pred)\n",
    "_disp = sm.ConfusionMatrixDisplay(confusion_matrix=_cm, display_labels=label_encoder.classes_)\n",
    "_disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Improved CNN Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mfOT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Try a hybrid CNN/RNN model.\n",
    "This should use convolutions to capture spatial features, and then use LSTM to capture temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vGiW",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_model = models.Sequential([\n",
    "    Input(INPUT_SHAPE),\n",
    "\n",
    "    # CNN feature extractor.\n",
    "    Conv1D(64, kernel_size=5, padding='same'),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Conv1D(128, kernel_size=5, padding='same'),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Conv1D(128, kernel_size=3, padding='same'),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    # Reshape for LSTM: [batch, reduced_timesteps, features]\n",
    "    Reshape((-1, 128)),\n",
    "\n",
    "    # Bidirectional LSTM.\n",
    "    Bidirectional(LSTM(128, dropout=0.3, return_sequences=True)),\n",
    "    Bidirectional(LSTM(64, dropout=0.3)),\n",
    "\n",
    "    # Classification head.\n",
    "    Dropout(0.5),\n",
    "    Dense(len(cleaned_unique_labels), activation='softmax')\n",
    "])\n",
    "\n",
    "# Usage\n",
    "cnn_rnn_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.AdamW(learning_rate=0.001), metrics=['accuracy'])\n",
    "cnn_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SYQT",
   "metadata": {},
   "outputs": [],
   "source": [
    "_history = cnn_rnn_model.fit(x_train_tensor, label_encoder.transform(y_train_tensor), epochs=EPOCHS)\n",
    "_y_pred_probability = cnn_rnn_model.predict(x_test_tensor)\n",
    "_y_pred = label_encoder.inverse_transform(np.argmax(_y_pred_probability, axis=1))\n",
    "\n",
    "_precision_score = sm.precision_score(y_test_tensor, _y_pred, average=None)\n",
    "_recall_score = sm.recall_score(y_test_tensor, _y_pred, average=None)\n",
    "for _i, _label in enumerate(cleaned_unique_labels):\n",
    "    print(f'For label {_label}: Precision {_precision_score[_i]:.2f}, Recall {_recall_score[_i]:.2f}')\n",
    "\n",
    "_cm = sm.confusion_matrix(y_test_tensor, _y_pred)\n",
    "_disp = sm.ConfusionMatrixDisplay(confusion_matrix=_cm, display_labels=label_encoder.classes_)\n",
    "_disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('CNN-RNN Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
