{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcfDOLPGbbD"
      },
      "source": [
        "# COMP6246 Machine Learning Technologies (2025/26)\n",
        "# Lab 5 – Perceptrons, Deep Net, and Convolutional Neural Net\n",
        "\n",
        "In this lab, we introduce how to implement a perceptron, a deep neural network (DNN) and also a convolutional neural network (CNN). We also present you with code that is working, but yields poor results. We expect you to spot the issues and improve the code. Exercises are also provided at the end of each section to improve your technical skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2o93U7kGbbG"
      },
      "source": [
        "## Setup\n",
        "\n",
        "_Make sure that the following code is executed before every other sections of this lab_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLgijH5GGbbI"
      },
      "outputs": [],
      "source": [
        "# Common imports\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "\n",
        "# To plot nice figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Clear tensorflow's and reset seed\n",
        "def reset_graph(seed=None):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxdkWQheGbbO"
      },
      "source": [
        "## A Perceptron\n",
        "\n",
        "In this section, we will use an artificial neuron (aka _perceptron_) to perform binary classification on linearly separable data. Specifically, we will use a portion of the Iris dataset; the description of this dataset can be found at <a href=\"http://scikit-learn.org/stable/datasets/index.html#iris-dataset\">http://scikit-learn.org/stable/datasets/index.html#iris-dataset</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ILxvq1yGbbP"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# get dataset\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)]  # use only petal length and petal width\n",
        "y = (iris.target == 0).astype(int) # classify them as either setosa or not setosa\n",
        "\n",
        "# visualise the data\n",
        "axes = [0, 5, 0, 2]\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\", fontsize=14)\n",
        "plt.axis(axes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00nQTKUyGbbY"
      },
      "source": [
        "Clearly, this task can be easily done by using a linear classifier. Could you visualise the linear decision boundary on the figure above? Where should it be?\n",
        "\n",
        "Now, let's move on to implementing a perceptron by using Scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqFSWaPsGbbZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# initialise and train a perceptron\n",
        "pct = Perceptron(max_iter=100, random_state=None)\n",
        "pct.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALYJyAB5Gbbf"
      },
      "source": [
        "Notice that there are many parameters that you can tweak here. You can have a look at the description of each parameter in the Scikit-Learn's documentation <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html</a>\n",
        "\n",
        "Next, we will extract the decision boundary from the model. Below we show a general way of extracting a decision boundary with any model. Note that it can be very computationally expensive if the feature space is large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eJ4gsNBGbbg"
      },
      "outputs": [],
      "source": [
        "# sampling and predict the whole space of features\n",
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(axes[0], axes[1], 100).reshape(-1, 1),\n",
        "        np.linspace(axes[2], axes[3], 100).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "y_predict = pct.predict(X_new)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "# plot the datapoints again\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
        "\n",
        "# get a nice color\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
        "\n",
        "# plot the predicted samples of feature space\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\", fontsize=14)\n",
        "plt.axis(axes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PqlSqiYGbbj"
      },
      "source": [
        "**_Exercise 1_**\n",
        "1. The decision boundary of a single perceptron is a single straight line, but the above plot shows differently! Fix this plot. (_Hint_: you need to sample the feature space more. Change the parameter of `np.linspace` above to generate more points. Have a look at the documentation: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html.)\n",
        "\n",
        "2. Try running the code in [3] and [4] multiple times; two snippets above where a network is initialised, trained, and plotted. Do you always get the same decision boundary? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il3Ml-yFGbbk"
      },
      "source": [
        "## Activation Functions\n",
        "\n",
        "There are many activation functions that can be used in a neuron. Different functions result in different behaviours, and consequently different pros & cons. Some of the popular activation functions are defined below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDpAEXMKGbbk"
      },
      "source": [
        "$$ \\text{heaviside} (z) = \\begin{cases} 1 & \\quad \\text{if } z >= 0 \\\\ 0 & \\quad \\text{otherwise} \\end{cases} $$\n",
        "\n",
        "$$ \\text{logit} (z) = \\frac{1}{1 + e^{-z}} $$\n",
        "\n",
        "$$ \\text{relu} (z) = \\max{\\left( 0 , z \\right)} $$\n",
        "\n",
        "$$ \\text{leaky_relu} (z, \\alpha) = \\max{\\left( \\alpha z , z \\right)} $$\n",
        "\n",
        "$$ \\text{elu} (z, \\alpha) = \\begin{cases} \\alpha \\left( e^z - 1 \\right) & \\quad \\text{if } z < 0 \\\\ z & \\quad \\text{otherwise} \\end{cases} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcyamp_aLRuo"
      },
      "source": [
        "**_Exercise 2_**\n",
        "Complete the cell below with the code for the activation functions listed (see equations). Note that they must be able to process NumPy arrays as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scufj5AzGbbl"
      },
      "outputs": [],
      "source": [
        "def heaviside(z):\n",
        "    return (z >= 0).astype(z.dtype)\n",
        "\n",
        "def logit(z): # modify this function. Hint: Use np.exp()\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def relu(z): # modify this function. Hint: Use np.maximum()\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def leaky_relu(z, alpha): # modify this function and set default alpha to 0.01\n",
        "    return np.maximum(alpha*z, z)\n",
        "\n",
        "def elu(z, alpha=1): # No need to modify this function!\n",
        "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
        "\n",
        "def selu(z, # No need to modify this function!\n",
        "         scale=1.0507009873554804934193349852946,\n",
        "         alpha=1.6732632423543772848170429916717):\n",
        "    return scale * elu(z, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PCH78xKZwZo"
      },
      "source": [
        "Now we plot these different activation functions using the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhQbob21Gbbq"
      },
      "outputs": [],
      "source": [
        "z = np.linspace(-5, 5, 200)\n",
        "\n",
        "plt.figure(figsize=(11,11))\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"Step\")\n",
        "plt.plot(z, np.tanh(z), \"b:\", linewidth=2, label=\"Tanh\")\n",
        "plt.plot(z, heaviside(z), \"y--\", linewidth=2, label=\"Heaviside\")\n",
        "plt.plot(z, logit(z), \"g-.\", linewidth=2, label=\"Logit\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc=\"lower right\", fontsize=14)\n",
        "plt.title(\"Activation Functions\", fontsize=14)\n",
        "plt.axis([-5, 5, -1.2, 1.2])\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.plot(z, relu(z), \"m-\", linewidth=2, label=\"ReLU\")\n",
        "plt.plot(z, leaky_relu(z, 0.05), \"k:\", linewidth=2, label=\"Leaky_ReLU\")\n",
        "plt.plot(z, elu(z), \"y--\", linewidth=2, label=\"ELU\")\n",
        "plt.plot(z, selu(z), \"g-.\", linewidth=2, label=\"SELU\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc=\"lower right\", fontsize=14)\n",
        "plt.title(\"Activation Functions\", fontsize=14)\n",
        "plt.axis([-5, 5, -2, 2])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "002ZvgQbGbbv"
      },
      "source": [
        "You should be able to see the following characteristics from the graph:\n",
        "- Step function and Heaviside function are quite similar except for their output ranges.\n",
        "- Similarly, the hyperbolic tangent and the logit/sigmoidal function are nearly the same except for their output ranges.\n",
        "- Lastly, all variants of ReLU functions behave differently only when the input sum of a perceptron is lower than zero.\n",
        "\n",
        "Note that different functions have different sensitivity to the perceptron input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "linkHUdcGbb7"
      },
      "source": [
        "## (Deeper) Neural Net for MNIST on TensorFlow\n",
        "\n",
        "In this section, we will construct and train a _deeper_ neural network with TensorFlow to perform classification. To train a large number of neurons, we would generally need a large dataset. So we will use MNIST (https://keras.io/api/datasets/mnist/) from now on. Keras API provides some utility functions to fetch and load some common datasets like MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT_FZAsSGbb8"
      },
      "outputs": [],
      "source": [
        "# Load MNIST\n",
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "#We will also create validation set. Further, we #scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255.\n",
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wckP3wW5ZwZo"
      },
      "source": [
        "You can plot an image using Matplotlib's `imshow()` function, with a 'binary' color map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp6jViu4ZwZo"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X_train[1], cmap=\"binary\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "-DGiF_iXGbb_"
      },
      "source": [
        "Remember the `Sequential API` that we used earlier? We will now use it to build a DNN with hidden layers as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si2c6rJjZwZp"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(30,activation='relu'),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvrr873qGbcI"
      },
      "source": [
        "You can use methods such as `model.summary()` or `model.layers()` to retrieve the details of the network defined above. See documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "\n",
        "To train this network, we need to define a loss function and choose an optimiser. The `model.compile()` method will be used to specify the loss function and the optimiser we want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLWOEhanZwZp"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUqcR_3oZwZp"
      },
      "source": [
        "The model can be trained now. We simply need to use `model.fit()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDBF7JiCZwZp"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_VdtRPQGbcQ"
      },
      "source": [
        "Rerun the cell above multiple times to see how accurate our trained model is. You should be able to see that the resulted accuracy is low and the training takes a long time.\n",
        "\n",
        "**_Exercise 3_**\n",
        "Modify and tune the neural net such that the accuracy and training time is improved. You can try the following:\n",
        "- Change the structure of the network by adding/removing a hidden layer or increasing/reducing number of neurons.\n",
        "- Change the activation function of the hidden layers.\n",
        "- Choose different optimisation algorithms such as `adam` or `RMSprop`. See documentation: https://keras.io/api/optimizers/\n",
        "\n",
        "Do you observe any effect on the accuracy during the tuning? What is the best model that you can achieve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7jTYsTeZwZp"
      },
      "source": [
        "## Running a CNN with MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5FCXFfwGbcS"
      },
      "outputs": [],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
        "\n",
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train = (X_train - X_mean) / X_std\n",
        "X_valid = (X_valid - X_mean) / X_std\n",
        "X_test = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_valid = X_valid[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhTyjagLZwZp"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "DefaultConv2D = partial(tf.keras.layers.Conv2D,\n",
        "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    DefaultConv2D(filters=64, kernel_size=5, input_shape=[28, 28, 1]),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #DefaultConv2D(filters=128),\n",
        "    DefaultConv2D(filters=128),\n",
        "    #tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "    #DefaultConv2D(filters=256),\n",
        "    #DefaultConv2D(filters=256),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    #tf.keras.layers.Dense(units=128, activation='relu'),\n",
        "    #tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=10, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1(0.01)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coRVYcumZwZp"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
        "score = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:10] # pretend we have new images\n",
        "y_pred = model.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eEqnbgeGbcc"
      },
      "source": [
        "**_Exercise 4_**\n",
        "1. Visualise and/or draw on your paper this convolutional neural net to figure out its current structure.\n",
        "2. Tune the model such that the accuracy is acceptably good, the required memory is low, and the training time is small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIzCfGQWGbcc"
      },
      "source": [
        "## Overfitting\n",
        "\n",
        "'With 4 parameters I can fit an elephant and with 5 I can make him wiggle his trunk.' John von Neumann, _cited by Enrico Fermi in Nature 427_\n",
        "\n",
        "Do not forget that an overfitted model will not perform well in the real world. It is therefore important for you to know how to prevent this issue with neural networks in general.\n",
        "\n",
        "**_Exercise 5_**\n",
        "\n",
        "1. On deep net and/or CNN for MNIST above, implement one or a combination of the regularisation techniques listed below. Observe any difference or change in performance during training:\n",
        "   \n",
        "   1.1. $l_1$ or $l_2$ regularisation, by correctly specifying TensorFlow parameters. (_Hint_: Look for `tf.keras.regularizers` in the online documentation)\n",
        "   \n",
        "   1.2. Dropout, where each neuron has a probability of being turned off at each epoch in training phase (_Hint_: apply `tf.keras.layers.Dropout()` (See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) to the input layer and/or any hidden layer's output, but NOT the output of the output layer. You could also use `tf.keras.layers.AlphaDropout()` (See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/AlphaDropout).)\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aGfVCIzGbcc"
      },
      "source": [
        "## Summary\n",
        "We have implemented DNN and CNN using tensorflow. There are still a lot of nuances that you can work with. This is just an overview to help you get started and build upon. Please refer to relevant Chapters in the refernce textbook to know more.\n",
        "\n",
        "## Sidenote\n",
        "There are many high level APIs that you can use to quickly create and deploy Machine Learning prototypes. They are very useful but it is difficult to make non-standard changes to their implementation of Machine Learning models. If you are interested, have a look on the following:\n",
        "- Estimators: <a href=\"https://www.tensorflow.org/guide/estimators\">https://www.tensorflow.org/guide/estimators</a>\n",
        "- Keras: <a href=\"https://www.tensorflow.org/guide/keras\">https://www.tensorflow.org/guide/keras</a>\n",
        "- Eager execution: <a href=\"https://www.tensorflow.org/guide/eager\">https://www.tensorflow.org/guide/eager</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic5qgx9cGbce"
      },
      "source": [
        "## Reference\n",
        "Aurélien Géron, _Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}